reading comprehension model by transformer
- The Architecture of this model employed the **transformer feature** parallized attention + **BiDAF query-wise Passage content state** + **PointerNetwork**. 


- train Loss:![loss](https://github.com/fooSynaptic/transfromer_NN_Block/blob/master/images/rc_model_train_loss.png)

- You may want to inspect the predict result in [here](https://github.com/fooSynaptic/transfromer_NN_Block/blob/master/results/rc_model_epoch_50_gs_10500)

final_reuslt: **`Rouge-L:0.2651. BLEU_1: 0.36.`**

Result is keep updating, welcome to star and follow.
